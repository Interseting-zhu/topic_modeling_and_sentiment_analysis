{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFvuGXs_cVRy",
        "outputId": "5e1c4a22-bb51-4e75-cbba-9c37d9843462"
      },
      "source": [
        "!python -m spacy download en_core_web_lg  #spacy词形还原模型，首次执行后需重新启动代码执行程序"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (54.2.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.4.1)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp37-none-any.whl size=829180944 sha256=715a340435d7f8587a65d853fbea08f8dc89799b3d2e7e95d3862330fea0c381\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ea4gyj20/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF01NOMgHxgc"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqHq7HNHMn9z",
        "outputId": "a4b8da43-b42f-40a5-8d9d-5998a14342b6"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kJkbo1_5Rvm"
      },
      "source": [
        "# 去停用词\n",
        "remove_list = ['but', 'few', 'no', 'nor', 'not', 'don', \"don't\", 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn',\n",
        "        \"didn't\", \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",\n",
        "        'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
        "        \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] #将这些词从停用词表中删除\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update('u','rt') #增加两个停用词：rt和u\n",
        "\n",
        "for word in remove_list:\n",
        "  stop_words.remove(word)\n",
        "\n",
        "def remove_urls(s): #去超链接\n",
        "    # re.sub(pattern,repl,string) is used to replace substrings. Will replace the matches in string with repl\n",
        "    return re.sub(r\"https?://\\S+\", \"\", s)\n",
        "\n",
        "def clean_txt(input_text):\n",
        "    input_text = str(input_text)\n",
        "    if type(input_text) != str:\n",
        "        return input_text\n",
        "    # removing hashtags,emojis,hyperlinks\n",
        "    input_text = re.sub(r\"#+\", \"\", str(input_text)) #只去掉'#'号\n",
        "    input_text = input_text.encode(\"ascii\", \"ignore\") #去ASCII码（表情）\n",
        "    input_text = input_text.decode()\n",
        "    input_text = remove_urls(input_text)   #去超链接     \n",
        "    input_text = re.sub(\"amp\", \"\", input_text) #去掉'amp'\n",
        "    input_text = re.sub(\"nan\", \"\", input_text) #去掉'nan'\n",
        "\n",
        "    # removing @user\n",
        "    r = re.findall(r\"@[\\w]*\", input_text)\n",
        "    for i in r:\n",
        "      input_text = re.sub(i, \"\", input_text)\n",
        "\n",
        "    input_text = \" \".join([i for i in wordpunct_tokenize(input_text) if not i.lower() in stop_words]) #如果某单词的小写形式在停用词表中，则去掉该词\n",
        "\n",
        "    # 去标点符号\n",
        "    result = re.sub(r\"[^\\w\\s]\", \"\", input_text)\n",
        "    \n",
        "    # removing links\n",
        "    #rresultes = re.sub(r\"https[\\w]*\", \"\", result, flags=re.MULTILINE)\n",
        "\n",
        "    # 去数字\n",
        "    result = \"\".join(i for i in result if not i.isdigit())\n",
        "    \n",
        "    # 去空格\n",
        "    result = re.sub(\"\\s+\", \" \", result)\n",
        "    if result[0] == \" \":\n",
        "      result = result[1:]\n",
        "    if result[-1] == \" \":\n",
        "      result = result[:-1]\n",
        "      \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JeWYoVl58Cl"
      },
      "source": [
        "def lemmatize_sentence_nltk(sentence):  #用NLTK进行词形还原（效果不佳，弃用）\n",
        "  wnl=WordNetLemmatizer()\n",
        "  new_sentence = []\n",
        "  for word,tag in pos_tag(nltk.word_tokenize(sentence)): #分词+词性标注\n",
        "    lemmatized = \"\"\n",
        "    if word.istitle():\n",
        "      if tag.startswith('NN'):\n",
        "          lemmatized = wnl.lemmatize(word.lower(), pos='n')\n",
        "      elif tag.startswith('VB'):\n",
        "          lemmatized = wnl.lemmatize(word.lower(), pos='v')\n",
        "      elif tag.startswith('JJ'):\n",
        "          lemmatized = wnl.lemmatize(word.lower(), pos='a')\n",
        "      elif tag.startswith('R'):\n",
        "          lemmatized = wnl.lemmatize(word.lower(), pos='r')\n",
        "      else:\n",
        "          lemmatized = word\n",
        "\n",
        "      lemmatized = lemmatized.capitalize()\n",
        "    else:\n",
        "      if tag.startswith('NN'):\n",
        "          lemmatized = wnl.lemmatize(word, pos='n')\n",
        "      elif tag.startswith('VB'):\n",
        "          lemmatized = wnl.lemmatize(word, pos='v')\n",
        "      elif tag.startswith('JJ'):\n",
        "          lemmatized = wnl.lemmatize(word, pos='a')\n",
        "      elif tag.startswith('R'):\n",
        "          lemmatized = wnl.lemmatize(word, pos='r')\n",
        "      else:\n",
        "          lemmatized = word\n",
        "\n",
        "    new_sentence.append(lemmatized)\n",
        "\n",
        "  new_sentence = \" \".join(new_sentence)\n",
        "  return new_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uzgeze97Cwa"
      },
      "source": [
        "def get_cleaned_data(df): #数据清理\n",
        "  t1 = time.time()\n",
        "  print('正在进行数据清理...')\n",
        "  df['content'] = df.content.apply(lambda x:clean_txt(x))\n",
        "  t2 = time.time()\n",
        "  print('数据清理耗时',t2-t1,'s')\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O4CtLGrlheY"
      },
      "source": [
        "from joblib import Parallel, delayed #用joblib加速词形还原\n",
        "\n",
        "def lemmatize_pipe(doc):\n",
        "  lemma_list = [tok.lemma_.upper() if tok.is_upper else tok.lemma_ for tok in doc] \n",
        "\n",
        "  return lemma_list\n",
        "\n",
        "def chunker(iterable, total_length, chunksize):\n",
        "  return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n",
        "\n",
        "def flatten(list_of_lists):\n",
        "  \"Flatten a list of lists to a combined list\"\n",
        "  return [item for sublist in list_of_lists for item in sublist]\n",
        "\n",
        "def process_chunk(texts):\n",
        "  preproc_pipe = []\n",
        "  for doc in nlp.pipe(texts, batch_size=128): #用spacy.pipe加速\n",
        "      preproc_pipe.append(lemmatize_pipe(doc))\n",
        "  return preproc_pipe\n",
        "\n",
        "def preprocess_parallel(texts, chunksize=1000): #加速\n",
        "  executor = Parallel(n_jobs=7, backend='multiprocessing', prefer=\"processes\")\n",
        "  do = delayed(process_chunk)\n",
        "  tasks = (do(chunk) for chunk in chunker(texts, len(texts), chunksize=chunksize))\n",
        "  result = executor(tasks)\n",
        "  return flatten(result)\n",
        "\n",
        "def lemmatize_sentence_spacy(df):  #用spaCy进行词形还原\n",
        "  t1 = time.time()\n",
        "  print('正在进行词形还原...')\n",
        "\n",
        "  docs = preprocess_parallel(df.content, chunksize=1000)\n",
        "  lemmatized_docs = []\n",
        "\n",
        "  for sentence in docs:\n",
        "    lemmatized = \" \".join(sentence)\n",
        "    lemmatized_docs.append(lemmatized)\n",
        "\n",
        "  df['content'] = lemmatized_docs\n",
        "\n",
        "  t2 = time.time()\n",
        "  print('词形还原耗时',t2-t1,'s')\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rOemjuLftlYe"
      },
      "source": [
        "def extract_data(df): #数据归约，从原始数据中提取有用数据\n",
        "  columns1 = ['date','content','user','replyCount','retweetCount','likeCount','quoteCount'] #日期与时间，文本内容，用户信息，回复数，转发数，点赞数，引用数\n",
        "  \n",
        "  t1 = time.time()\n",
        "  print('正在进行数据归约...')\n",
        "  \n",
        "  df = df[columns1].copy()\n",
        " \n",
        "  df['length']=df.content.apply(lambda x:len(x))      #文本长度\n",
        "  df['date']=df.date.apply(lambda x:x[:10])         #日期\n",
        "  df['year']=df.date.apply(lambda x:x.split(\"-\")[0])     #年份\n",
        "  df['month']=df.date.apply(lambda x:x.split(\"-\")[1])    #月份\n",
        "  df['day']=df.date.apply(lambda x:x.split(\"-\")[2])     #日\n",
        "  df['username']=df.user.apply(lambda x:eval(x)['username']) #用户名\n",
        "  df['location']=df.user.apply(lambda x:eval(x)['location']) #地点(不一定精确)\n",
        " \n",
        "  t2 = time.time()\n",
        "  print('数据归约耗时',t2-t1,'s')\n",
        "\n",
        "  columns2 = ['date','year','month','day','content','length','replyCount','retweetCount','likeCount','quoteCount','username','location']\n",
        "  \n",
        "  return df[columns2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmcFIGmZfsv6"
      },
      "source": [
        "def get_preprocessed_data(path):\n",
        "  print('文件路径：',path)\n",
        "  df = pd.read_pickle(path)\n",
        "  df = df.drop_duplicates(subset=['content'],keep='first',ignore_index=True) #文本去重\n",
        "  df = get_cleaned_data(df)\n",
        "  df = lemmatize_sentence_spacy(df)\n",
        "  df = extract_data(df)\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzyATiMsnsxc",
        "outputId": "8afe3d30-266d-4a04-996f-ff4a18bf6f10"
      },
      "source": [
        "df = get_preprocessed_data(\"/content/drive/My Drive/202012.pkl\") #读取pkl格式文件，读取速度更快"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "文件路径： /content/drive/My Drive/202012.pkl\n",
            "正在进行数据清理...\n",
            "数据清理耗时 306.22693848609924 s\n",
            "正在进行词形还原...\n",
            "词形还原耗时 2445.876158952713 s\n",
            "正在进行数据归约...\n",
            "数据归约耗时 157.14918875694275 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGrJqaVkYYg9"
      },
      "source": [
        "df.sort_values(by='date',inplace=True,ignore_index=True) #按日期排序，并重建行索引"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "d9ct0TKIYyWX",
        "outputId": "e0a7318f-06d1-452f-edc5-a1db16e3e2e5"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>content</th>\n",
              "      <th>length</th>\n",
              "      <th>replyCount</th>\n",
              "      <th>retweetCount</th>\n",
              "      <th>likeCount</th>\n",
              "      <th>quoteCount</th>\n",
              "      <th>username</th>\n",
              "      <th>location</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-12-01</td>\n",
              "      <td>2020</td>\n",
              "      <td>12</td>\n",
              "      <td>01</td>\n",
              "      <td>do bit keep patient staff safe well tuesdayfri...</td>\n",
              "      <td>59</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>gabyvparker</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-12-01</td>\n",
              "      <td>2020</td>\n",
              "      <td>12</td>\n",
              "      <td>01</td>\n",
              "      <td>Battle Axe Division cover sixth leg Konark Cor...</td>\n",
              "      <td>234</td>\n",
              "      <td>8</td>\n",
              "      <td>197</td>\n",
              "      <td>848</td>\n",
              "      <td>3</td>\n",
              "      <td>IaSouthern</td>\n",
              "      <td>Pune, India</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-12-01</td>\n",
              "      <td>2020</td>\n",
              "      <td>12</td>\n",
              "      <td>01</td>\n",
              "      <td>pandemic happen regularly afford pretend don C...</td>\n",
              "      <td>66</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>taggcleanhands</td>\n",
              "      <td>Toronto, Ontario</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-12-01</td>\n",
              "      <td>2020</td>\n",
              "      <td>12</td>\n",
              "      <td>01</td>\n",
              "      <td>read entrail use wastewater epidemiology track...</td>\n",
              "      <td>93</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>RSCTheAcademies</td>\n",
              "      <td>Ottawa - Canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-12-01</td>\n",
              "      <td>2020</td>\n",
              "      <td>12</td>\n",
              "      <td>01</td>\n",
              "      <td>RGGI Next TSNP News Today Resgreen Group compl...</td>\n",
              "      <td>216</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>stockpickmover</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1112415</th>\n",
              "      <td>2020-12-31</td>\n",
              "      <td>2020</td>\n",
              "      <td>12</td>\n",
              "      <td>31</td>\n",
              "      <td>beat track also report pandemic force large nu...</td>\n",
              "      <td>165</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>soutikBBC</td>\n",
              "      <td>BBC News</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1112416</th>\n",
              "      <td>2020-12-31</td>\n",
              "      <td>2020</td>\n",
              "      <td>12</td>\n",
              "      <td>31</td>\n",
              "      <td>June tell story miracle recovery Nitaidas Mukh...</td>\n",
              "      <td>165</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>soutikBBC</td>\n",
              "      <td>BBC News</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1112417</th>\n",
              "      <td>2020-12-31</td>\n",
              "      <td>2020</td>\n",
              "      <td>12</td>\n",
              "      <td>31</td>\n",
              "      <td>July write whether India enough oxygen supply ...</td>\n",
              "      <td>123</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>soutikBBC</td>\n",
              "      <td>BBC News</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1112418</th>\n",
              "      <td>2020-12-31</td>\n",
              "      <td>2020</td>\n",
              "      <td>12</td>\n",
              "      <td>31</td>\n",
              "      <td>avoid zombification COVID consumption game cha...</td>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>rdomenechv</td>\n",
              "      <td>Spain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1112419</th>\n",
              "      <td>2020-12-31</td>\n",
              "      <td>2020</td>\n",
              "      <td>12</td>\n",
              "      <td>31</td>\n",
              "      <td>world pretty good even though many infect covi...</td>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>justgeorgeous2</td>\n",
              "      <td>Perth, Western Australia</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1112420 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               date  year  ...         username                  location\n",
              "0        2020-12-01  2020  ...      gabyvparker                          \n",
              "1        2020-12-01  2020  ...       IaSouthern               Pune, India\n",
              "2        2020-12-01  2020  ...   taggcleanhands          Toronto, Ontario\n",
              "3        2020-12-01  2020  ...  RSCTheAcademies           Ottawa - Canada\n",
              "4        2020-12-01  2020  ...   stockpickmover                          \n",
              "...             ...   ...  ...              ...                       ...\n",
              "1112415  2020-12-31  2020  ...        soutikBBC                  BBC News\n",
              "1112416  2020-12-31  2020  ...        soutikBBC                  BBC News\n",
              "1112417  2020-12-31  2020  ...        soutikBBC                  BBC News\n",
              "1112418  2020-12-31  2020  ...       rdomenechv                     Spain\n",
              "1112419  2020-12-31  2020  ...   justgeorgeous2  Perth, Western Australia\n",
              "\n",
              "[1112420 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnXrLfFMLmXv"
      },
      "source": [
        "df.to_pickle('/content/drive/My Drive/202012_processed.pkl') #保存为pkl格式文件"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "psXEEGx3FoRX",
        "outputId": "e2660c0d-727b-48b9-eeec-6c336edc1d2b"
      },
      "source": [
        "x.content[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Lower than vermin, @BBCNews @BBCNewsnight @BBCPolitics @BBCr4today @BBCWorldatOne @BBCPM @Channel4News #QED #ToryScum #bbcaq #ToryCovidCatastrophe #COVID19 #murderers #LabourLeaks #LabourLeadershipElection2021 #FordeReport #LabourScum #Fascist #ApartheidIsrael  #StarmerOut https://t.co/mrIKMoifTA'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NkyUQAOpFl7R",
        "outputId": "da9f1e95-7e6d-454f-a492-b595385c6b2e"
      },
      "source": [
        "df.content[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'low vermin night QED ToryScum bbcaq torycovidcatastrophe COVID murderer LabourLeaks LabourLeadershipElection FordeReport LabourScum Fascist ApartheidIsrael StarmerOut'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    }
  ]
}